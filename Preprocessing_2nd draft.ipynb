{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## preprocessing - (to3tuple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Id</th>\n",
       "      <th>ProductId</th>\n",
       "      <th>UserId</th>\n",
       "      <th>ProfileName</th>\n",
       "      <th>HelpfulnessNumerator</th>\n",
       "      <th>HelpfulnessDenominator</th>\n",
       "      <th>Score</th>\n",
       "      <th>Time</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "      <th>sa_compound_score</th>\n",
       "      <th>score_with_sa</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>B001E4KFG0</td>\n",
       "      <td>A3SGXH7AUHU8GW</td>\n",
       "      <td>delmartian</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1303862400</td>\n",
       "      <td>Good Quality Dog Food</td>\n",
       "      <td>I have bought several of the Vitality canned d...</td>\n",
       "      <td>0.9441</td>\n",
       "      <td>4.0559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>B00813GRG4</td>\n",
       "      <td>A1D87F6ZCVE5NK</td>\n",
       "      <td>dll pa</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1346976000</td>\n",
       "      <td>Not as Advertised</td>\n",
       "      <td>Product arrived labeled as Jumbo Salted Peanut...</td>\n",
       "      <td>-0.5664</td>\n",
       "      <td>1.5664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>B000LQOCH0</td>\n",
       "      <td>ABXLMWJIXXAIN</td>\n",
       "      <td>Natalia Corres \"Natalia Corres\"</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1219017600</td>\n",
       "      <td>\"Delight\" says it all</td>\n",
       "      <td>This is a confection that has been around a fe...</td>\n",
       "      <td>0.8265</td>\n",
       "      <td>3.1735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>B000UA0QIQ</td>\n",
       "      <td>A395BORC6FGVXV</td>\n",
       "      <td>Karl</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1307923200</td>\n",
       "      <td>Cough Medicine</td>\n",
       "      <td>If you are looking for the secret ingredient i...</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>2.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>B006K2ZZ7K</td>\n",
       "      <td>A1UQRSCLF8GW1T</td>\n",
       "      <td>Michael D. Bigham \"M. Wassir\"</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1350777600</td>\n",
       "      <td>Great taffy</td>\n",
       "      <td>Great taffy at a great price.  There was a wid...</td>\n",
       "      <td>0.9468</td>\n",
       "      <td>4.0532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568449</th>\n",
       "      <td>568449</td>\n",
       "      <td>568450</td>\n",
       "      <td>B001EO7N10</td>\n",
       "      <td>A28KG5XORO54AY</td>\n",
       "      <td>Lettie D. Carter</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1299628800</td>\n",
       "      <td>Will not do without</td>\n",
       "      <td>Great for sesame chicken..this is a good if no...</td>\n",
       "      <td>0.8589</td>\n",
       "      <td>4.1411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568450</th>\n",
       "      <td>568450</td>\n",
       "      <td>568451</td>\n",
       "      <td>B003S1WTCU</td>\n",
       "      <td>A3I8AFVPEE8KI5</td>\n",
       "      <td>R. Sawyer</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1331251200</td>\n",
       "      <td>disappointed</td>\n",
       "      <td>I'm disappointed with the flavor. The chocolat...</td>\n",
       "      <td>-0.4848</td>\n",
       "      <td>2.4848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568451</th>\n",
       "      <td>568451</td>\n",
       "      <td>568452</td>\n",
       "      <td>B004I613EE</td>\n",
       "      <td>A121AA1GQV751Z</td>\n",
       "      <td>pksd \"pk_007\"</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>1329782400</td>\n",
       "      <td>Perfect for our maltipoo</td>\n",
       "      <td>These stars are small, so you can give 10-15 o...</td>\n",
       "      <td>0.4352</td>\n",
       "      <td>4.5648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568452</th>\n",
       "      <td>568452</td>\n",
       "      <td>568453</td>\n",
       "      <td>B004I613EE</td>\n",
       "      <td>A3IBEVCTXKNOH</td>\n",
       "      <td>Kathy A. Welch \"katwel\"</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1331596800</td>\n",
       "      <td>Favorite Training and reward treat</td>\n",
       "      <td>These are the BEST treats for training and rew...</td>\n",
       "      <td>0.9717</td>\n",
       "      <td>4.0283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568453</th>\n",
       "      <td>568453</td>\n",
       "      <td>568454</td>\n",
       "      <td>B001LR2CU2</td>\n",
       "      <td>A3LGQPJCZVL9UC</td>\n",
       "      <td>srfell17</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1338422400</td>\n",
       "      <td>Great Honey</td>\n",
       "      <td>I am very satisfied ,product is as advertised,...</td>\n",
       "      <td>0.4754</td>\n",
       "      <td>4.5246</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>568454 rows Ã— 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Unnamed: 0      Id   ProductId          UserId  \\\n",
       "0                0       1  B001E4KFG0  A3SGXH7AUHU8GW   \n",
       "1                1       2  B00813GRG4  A1D87F6ZCVE5NK   \n",
       "2                2       3  B000LQOCH0   ABXLMWJIXXAIN   \n",
       "3                3       4  B000UA0QIQ  A395BORC6FGVXV   \n",
       "4                4       5  B006K2ZZ7K  A1UQRSCLF8GW1T   \n",
       "...            ...     ...         ...             ...   \n",
       "568449      568449  568450  B001EO7N10  A28KG5XORO54AY   \n",
       "568450      568450  568451  B003S1WTCU  A3I8AFVPEE8KI5   \n",
       "568451      568451  568452  B004I613EE  A121AA1GQV751Z   \n",
       "568452      568452  568453  B004I613EE   A3IBEVCTXKNOH   \n",
       "568453      568453  568454  B001LR2CU2  A3LGQPJCZVL9UC   \n",
       "\n",
       "                            ProfileName  HelpfulnessNumerator  \\\n",
       "0                            delmartian                     1   \n",
       "1                                dll pa                     0   \n",
       "2       Natalia Corres \"Natalia Corres\"                     1   \n",
       "3                                  Karl                     3   \n",
       "4         Michael D. Bigham \"M. Wassir\"                     0   \n",
       "...                                 ...                   ...   \n",
       "568449                 Lettie D. Carter                     0   \n",
       "568450                        R. Sawyer                     0   \n",
       "568451                    pksd \"pk_007\"                     2   \n",
       "568452          Kathy A. Welch \"katwel\"                     1   \n",
       "568453                         srfell17                     0   \n",
       "\n",
       "        HelpfulnessDenominator  Score        Time  \\\n",
       "0                            1      5  1303862400   \n",
       "1                            0      1  1346976000   \n",
       "2                            1      4  1219017600   \n",
       "3                            3      2  1307923200   \n",
       "4                            0      5  1350777600   \n",
       "...                        ...    ...         ...   \n",
       "568449                       0      5  1299628800   \n",
       "568450                       0      2  1331251200   \n",
       "568451                       2      5  1329782400   \n",
       "568452                       1      5  1331596800   \n",
       "568453                       0      5  1338422400   \n",
       "\n",
       "                                   Summary  \\\n",
       "0                    Good Quality Dog Food   \n",
       "1                        Not as Advertised   \n",
       "2                    \"Delight\" says it all   \n",
       "3                           Cough Medicine   \n",
       "4                              Great taffy   \n",
       "...                                    ...   \n",
       "568449                 Will not do without   \n",
       "568450                        disappointed   \n",
       "568451            Perfect for our maltipoo   \n",
       "568452  Favorite Training and reward treat   \n",
       "568453                         Great Honey   \n",
       "\n",
       "                                                     Text  sa_compound_score  \\\n",
       "0       I have bought several of the Vitality canned d...             0.9441   \n",
       "1       Product arrived labeled as Jumbo Salted Peanut...            -0.5664   \n",
       "2       This is a confection that has been around a fe...             0.8265   \n",
       "3       If you are looking for the secret ingredient i...             0.0000   \n",
       "4       Great taffy at a great price.  There was a wid...             0.9468   \n",
       "...                                                   ...                ...   \n",
       "568449  Great for sesame chicken..this is a good if no...             0.8589   \n",
       "568450  I'm disappointed with the flavor. The chocolat...            -0.4848   \n",
       "568451  These stars are small, so you can give 10-15 o...             0.4352   \n",
       "568452  These are the BEST treats for training and rew...             0.9717   \n",
       "568453  I am very satisfied ,product is as advertised,...             0.4754   \n",
       "\n",
       "        score_with_sa  \n",
       "0              4.0559  \n",
       "1              1.5664  \n",
       "2              3.1735  \n",
       "3              2.0000  \n",
       "4              4.0532  \n",
       "...               ...  \n",
       "568449         4.1411  \n",
       "568450         2.4848  \n",
       "568451         4.5648  \n",
       "568452         4.0283  \n",
       "568453         4.5246  \n",
       "\n",
       "[568454 rows x 13 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "df = pd.read_csv('Reviews_sa.csv')\n",
    "df['score_with_sa'] = df['Score']-df['sa_compound_score']\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## generate ratings\n",
    "users = df['UserId'].unique()\n",
    "items = df['ProductId'].unique()\n",
    "users_dict = {users[i]:i for i in range(len(users))}\n",
    "items_dict = {items[i]:i for i in range(len(items))}\n",
    "intUserId = df.apply(lambda x: users_dict[x['UserId']], axis=1)\n",
    "intItemId = df.apply(lambda x: items_dict[x['ProductId']], axis=1)\n",
    "ratings = pd.concat([intUserId,intItemId,df['Score']], axis=1)\n",
    "ratings.to_csv('Ratings.csv', index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_vip = df[df.groupby('UserId').UserId.transform(len) > 1]\n",
    "df_vip5 = df[df.groupby('UserId').UserId.transform(len) > 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7389902 5109819\n"
     ]
    }
   ],
   "source": [
    "#remove users appears once and generate ratings\n",
    "users = df_vip['UserId'].unique()\n",
    "items = df_vip['ProductId'].unique()\n",
    "users_dict = {users[i]:i for i in range(len(users))}\n",
    "items_dict = {items[i]:i for i in range(len(items))}\n",
    "intUserId = df_vip.apply(lambda x: users_dict[x['UserId']], axis=1)\n",
    "intItemId = df_vip.apply(lambda x: items_dict[x['ProductId']], axis=1)\n",
    "ratings = pd.concat([intUserId,intItemId,df_vip['Score']], axis=1)\n",
    "ratings.to_csv('Ratings_vip.csv', index=False, header=False)\n",
    "print(df.size, df_vip.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare for association rule\n",
    "bucket_dict = {}\n",
    "for index, row in df_vip.iterrows():\n",
    "    bucket_dict.setdefault(row[\"UserId\"], []).append(row[\"ProductId\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import math\n",
    "import random\n",
    "bucketTrain = open(\"bucket_train.csv\",\"w\")\n",
    "bucketTest = open(\"bucket_test.csv\",\"w\")\n",
    "for key, val in bucket_dict.items():\n",
    "    if random.random()<0.1:\n",
    "        if len(val)>20:\n",
    "            bucketTest.write('\\n'+','.join(val[:20]))\n",
    "        if len(val)<20:\n",
    "            nans = ['NaN']*(20-len(val))\n",
    "            bucketTest.write('\\n'+','.join(val)+','+','.join(nans))\n",
    "    else:\n",
    "        if len(val)>20:\n",
    "            bucketTrain.write('\\n'+','.join(val[:20]))\n",
    "        if len(val)<20:\n",
    "            nans = ['NaN']*(20-len(val))\n",
    "            bucketTrain.write('\\n'+','.join(val)+','+','.join(nans))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add in sentiment analysis\n",
    "users = df_vip['UserId'].unique()\n",
    "items = df_vip['ProductId'].unique()\n",
    "users_dict = {users[i]:i for i in range(len(users))}\n",
    "items_dict = {items[i]:i for i in range(len(items))}\n",
    "intUserId = df_vip.apply(lambda x: users_dict[x['UserId']], axis=1)\n",
    "intItemId = df_vip.apply(lambda x: items_dict[x['ProductId']], axis=1)\n",
    "ratings = pd.concat([intUserId,intItemId,df_vip['score_with_sa']], axis=1)\n",
    "ratings.to_csv('Ratings_vip_sa.csv', index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ch000\\.conda\\envs\\py3\\lib\\site-packages\\ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "C:\\Users\\Ch000\\.conda\\envs\\py3\\lib\\site-packages\\ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "C:\\Users\\Ch000\\.conda\\envs\\py3\\lib\\site-packages\\ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "#moderate user scores based on user input and group input and generate ratings\n",
    "df_vip['AvgUserScore'] = df_vip['Score'].groupby(df_vip['UserId']).transform('mean')\n",
    "df_vip['AvgProductScore'] = df_vip['Score'].groupby(df_vip['ProductId']).transform('mean')\n",
    "df_vip['ModeratedScore'] = 0.75*df_vip['AvgUserScore'] + 0.25*df_vip['AvgProductScore']\n",
    "users = df_vip['UserId'].unique()\n",
    "items = df_vip['ProductId'].unique()\n",
    "users_dict = {users[i]:i for i in range(len(users))}\n",
    "items_dict = {items[i]:i for i in range(len(items))}\n",
    "intUserId = df_vip.apply(lambda x: users_dict[x['UserId']], axis=1)\n",
    "intItemId = df_vip.apply(lambda x: items_dict[x['ProductId']], axis=1)\n",
    "ratings = pd.concat([intUserId,intItemId,df_vip['ModeratedScore']], axis=1)\n",
    "ratings.to_csv('Ratings_vip_moderateS.csv', index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7389902 2818686\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ch000\\.conda\\envs\\py3\\lib\\site-packages\\ipykernel_launcher.py:43: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\Ch000\\.conda\\envs\\py3\\lib\\site-packages\\ipykernel_launcher.py:44: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\Ch000\\.conda\\envs\\py3\\lib\\site-packages\\ipykernel_launcher.py:45: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    }
   ],
   "source": [
    "#remove users appears once and generate ratings\n",
    "users = df_vip5['UserId'].unique()\n",
    "items = df_vip5['ProductId'].unique()\n",
    "users_dict = {users[i]:i for i in range(len(users))}\n",
    "items_dict = {items[i]:i for i in range(len(items))}\n",
    "intUserId = df_vip5.apply(lambda x: users_dict[x['UserId']], axis=1)\n",
    "intItemId = df_vip5.apply(lambda x: items_dict[x['ProductId']], axis=1)\n",
    "ratings = pd.concat([intUserId,intItemId,df_vip5['Score']], axis=1)\n",
    "ratings.to_csv('Ratings_vip5.csv', index=False, header=False)\n",
    "print(df.size, df_vip5.size)\n",
    "# prepare for association rule\n",
    "bucket_dict = {}\n",
    "for index, row in df_vip5.iterrows():\n",
    "    bucket_dict.setdefault(row[\"UserId\"], []).append(row[\"ProductId\"])\n",
    "import csv\n",
    "import math\n",
    "import random\n",
    "bucketTrain = open(\"bucket_train_vip5.csv\",\"w\")\n",
    "bucketTest = open(\"bucket_test_vip5.csv\",\"w\")\n",
    "for key, val in bucket_dict.items():\n",
    "    if random.random()<0.1:\n",
    "        if len(val)>20:\n",
    "            bucketTest.write('\\n'+','.join(val[:20]))\n",
    "        if len(val)<20:\n",
    "            nans = ['NaN']*(20-len(val))\n",
    "            bucketTest.write('\\n'+','.join(val)+','+','.join(nans))\n",
    "    else:\n",
    "        if len(val)>20:\n",
    "            bucketTrain.write('\\n'+','.join(val[:20]))\n",
    "        if len(val)<20:\n",
    "            nans = ['NaN']*(20-len(val))\n",
    "            bucketTrain.write('\\n'+','.join(val)+','+','.join(nans))\n",
    "# add in sentiment analysis\n",
    "users = df_vip5['UserId'].unique()\n",
    "items = df_vip5['ProductId'].unique()\n",
    "users_dict = {users[i]:i for i in range(len(users))}\n",
    "items_dict = {items[i]:i for i in range(len(items))}\n",
    "intUserId = df_vip5.apply(lambda x: users_dict[x['UserId']], axis=1)\n",
    "intItemId = df_vip5.apply(lambda x: items_dict[x['ProductId']], axis=1)\n",
    "ratings = pd.concat([intUserId,intItemId,df_vip5['score_with_sa']], axis=1)\n",
    "ratings.to_csv('Ratings_vip5_sa.csv', index=False, header=False)\n",
    "#moderate user scores based on user input and group input and generate ratings\n",
    "df_vip5['AvgUserScore'] = df_vip5['Score'].groupby(df_vip5['UserId']).transform('mean')\n",
    "df_vip5['AvgProductScore'] = df_vip5['Score'].groupby(df_vip5['ProductId']).transform('mean')\n",
    "df_vip5['ModeratedScore'] = 0.75*df_vip5['AvgUserScore'] + 0.25*df_vip5['AvgProductScore']\n",
    "users = df_vip5['UserId'].unique()\n",
    "items = df_vip5['ProductId'].unique()\n",
    "users_dict = {users[i]:i for i in range(len(users))}\n",
    "items_dict = {items[i]:i for i in range(len(items))}\n",
    "intUserId = df_vip5.apply(lambda x: users_dict[x['UserId']], axis=1)\n",
    "intItemId = df_vip5.apply(lambda x: items_dict[x['ProductId']], axis=1)\n",
    "ratings = pd.concat([intUserId,intItemId,df_vip5['ModeratedScore']], axis=1)\n",
    "ratings.to_csv('Ratings_vip5_moderateS.csv', index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
